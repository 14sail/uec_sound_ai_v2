{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9872611-fa8c-4f4d-89dc-ebea772c16a8",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- 第一階段剪枝：Weight-Pruning\n",
    "- 用於alarm, moaning, enghlish-help, misc-sounds classifications\n",
    "- 剪枝比例：0.8-0.9, 建議0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "686e62d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_PATH = '/home/sail/sound_project/sound_ai_v2'\n",
    "step_now = 'STEP/step_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37c0093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_model_in_step1 = \"/home/sail/sound_project/sound_ai_v2/STEP/step_1/save_pt_model_s1/20240829_11/base4C_train_lr0.05_bs32_wd0.0005_20240829115502/uec_model_4Classes_hacc88.0_valacc_88.0_tracc_85.64453125_428th_epoch.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15eb05a5-0216-4c21-9226-27bca5bb4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "import sys;\n",
    "sys.path.append(os.path.abspath(f'{project_PATH}'))\n",
    "\n",
    "import glob;\n",
    "import math;\n",
    "import numpy as np;\n",
    "import random;\n",
    "import time;\n",
    "import torch\n",
    "import torch.optim as optim;\n",
    "import torch.nn as nn;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "886f8b70-1961-4c4f-823f-55393ff9c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utils as U;\n",
    "import common.opts as opt;\n",
    "# import th.resources.models as models;\n",
    "# import th.resources.calculator as calc;\n",
    "# import th.resources.train_generator as train_generator;\n",
    "import th.resources.pruning_tools.weight_pruning as weight_pruner;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd58c73b-11bf-48dc-895b-0959625f38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import common.tlopts as tlopts\n",
    "import th.resources.calculator as calc;\n",
    "from datetime import datetime;\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f89dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/home/sail/sound_project/sound_ai_v2/STEP/step_2/save_pt_model_s2/20240829_14' already exists.\n"
     ]
    }
   ],
   "source": [
    "date_time = datetime.now().strftime(\"%Y%m%d_%H\")  # %M\n",
    "save_pt_model_path = f'{project_PATH}/{step_now}/save_pt_model_s2/{date_time}'\n",
    "try:\n",
    "    os.mkdir(save_pt_model_path)\n",
    "    print(f\"Folder '{save_pt_model_path}' created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Folder '{save_pt_model_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e27f14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b81d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_len = 20150\n",
    "sr = 20150\n",
    "choose_class=[0,1,2,3,4,5,6]\n",
    "PATH = f'{project_PATH}/STEP/data_v2.npz'\n",
    "\n",
    "ch_n_class = 7\n",
    "fcn_no_of_inputs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c8d321e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0242209434509277, -3.1673121452331543)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(PATH, allow_pickle=True) \n",
    "audio_max_value =  float((data['sounds_train']).max())*2 # the max value of the sound samples\n",
    "audio_min_value =  float((data['sounds_train']).min())*2 # the max value of the sound samples\n",
    "audio_max_value, audio_min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae79f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [0,1,2,3,4,5,6]\n",
    "\n",
    "def one_hot_encode(data, order):\n",
    "    index_map = {number: index for index, number in enumerate(order)}\n",
    "    one_hot_list = []\n",
    "\n",
    "    for num in data:\n",
    "        one_hot = [0] * len(order)\n",
    "        if num in index_map:\n",
    "            one_hot[index_map[num]] = 1\n",
    "        one_hot_list.append(one_hot)\n",
    "\n",
    "    return one_hot_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bb31f54-0d24-4942-ad4c-f498900e61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log file object\n",
    "logObj = None;\n",
    "def ChkAndCreateSingleDir(dir_path):\n",
    "    if not pathlib.Path(dir_path).is_dir():\n",
    "        os.mkdir(dir_path);\n",
    "        print(f\"'{dir_path}' folder is created.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "944e5af4-56fa-45e2-8ef8-a1b3beb25156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDataTimeStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H:%M:%S').replace('-',\"\").replace(' ',\"\").replace(':',\"\");\n",
    "\n",
    "def getDateStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H').replace('-',\"\").replace(' ',\"\")#.replace(':',\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02f626e5-5ed6-4aef-908f-2a39342bb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(seed);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = {0:1, 1:2, 2:3, 3:4, 4:5, 5:6, 6:7,}\n",
    "        # dict([('52',1),('56',2),('99',3)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch_select_fixed_class(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[str(label1)]- 1\n",
    "            idx2 = self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        \n",
    "        return sounds, labels;\n",
    "\n",
    "    def generate_batch_select_fixed_class(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        #two variables recording alarm and moaning sounds count\n",
    "        alarm_selected, help_eng_selected, help_ch_selected, help_ja_selected, help_tw_selected, help_hk_selected = 0, 0, 0, 0, 0, 0     \n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                # print(\"enter while true\")\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                lbl1_int = np.int16(label1);\n",
    "                lbl2_int = np.int16(label2);\n",
    "                if label1 != label2:\n",
    "                    # {'alarm': 0, 'en_help': 1, 'ch_help': 2, 'ja_help': 3, 'tw_help': 4, 'hk_help': 5, 'Environment': 6, 'other': 7}\n",
    "                    if (lbl1_int == 0 and lbl2_int == 6) or (lbl1_int == 6 and lbl2_int ==0):\n",
    "                        if (alarm_selected == help_ch_selected) and (alarm_selected == help_eng_selected):\n",
    "                            alarm_selected += 1;\n",
    "                            break;\n",
    "                    if (lbl1_int == 1 and lbl2_int == 6) or (lbl1_int == 6 and lbl2_int == 1):\n",
    "                        if (help_eng_selected < alarm_selected) and (help_eng_selected == help_ch_selected):\n",
    "                            help_eng_selected += 1;\n",
    "                            break;\n",
    "                    if (lbl1_int == 2 and lbl2_int == 6) or (lbl1_int == 6 and lbl2_int == 2):\n",
    "                        if (help_ch_selected < alarm_selected) and (help_ch_selected < help_eng_selected):\n",
    "                            help_ch_selected += 1;\n",
    "                            break;\n",
    "                    if (lbl1_int == 3 and lbl2_int == 6) or (lbl1_int == 6 and lbl2_int == 3):\n",
    "                        if (help_ja_selected < alarm_selected) and (help_ja_selected < help_ch_selected):\n",
    "                            help_ja_selected += 1;\n",
    "                            break;\n",
    "                    if (lbl1_int == 4 and lbl2_int == 6) or (lbl1_int == 6 and lbl2_int == 4):\n",
    "                        if (help_tw_selected < alarm_selected) and (help_tw_selected < help_ch_selected):\n",
    "                            help_tw_selected += 1;\n",
    "                            break;\n",
    "                    if (lbl1_int == 5 and lbl2_int == 6) or (lbl1_int == 5 and lbl2_int == 4):\n",
    "                        if (help_hk_selected < alarm_selected) and (help_hk_selected < help_ch_selected):\n",
    "                            help_hk_selected += 1;\n",
    "                            break;\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random());\n",
    "            #######make wanted class mix ration above 0.5##########\n",
    "            # iLbl1 = np.int16(label1);\n",
    "            # iLbl2 = np.int16(label2);\n",
    "            # r = 1.0;\n",
    "            # p_ratio1 = 0.4\n",
    "            # p_ratio2 = 0.6\n",
    "            # while True:\n",
    "            #     r = np.array(random.random());\n",
    "            #     if r > p_ratio1 and iLbl1 != 99 :\n",
    "            #         break;\n",
    "            #     if r < p_ratio2 and iLbl2 != 99 :\n",
    "            #         break;\n",
    "            #######################End#######################\n",
    "            # print(f\"r:{r}, lbl1:{label1}, lbl2:{label2}\")  \n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = label1 # self.mapdict[str(label1)]- 1\n",
    "            idx2 = label2 # self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            \n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"batchIndex is {batchIndex}, total sounds is {len(sounds)}\")\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "        # print(f\"alarm_selected:{alarm_selected}, moaning_selected:{moaning_selected}\");\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.regularization(audio_max_value, audio_min_value)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d595d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1123;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12cdf441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customed_ACDNetV2(nn.Module):\n",
    "    def __init__(self, input_length, n_class, sr, ch_conf=None):\n",
    "        super(Customed_ACDNetV2, self).__init__()\n",
    "        self.input_length = input_length\n",
    "        self.ch_config = ch_conf\n",
    "\n",
    "        stride1 = 2\n",
    "        stride2 = 2\n",
    "        channels = 8\n",
    "        k_size = (3, 3)\n",
    "        n_frames = (sr / 1000) * 10  # No of frames per 10ms\n",
    "\n",
    "        sfeb_pool_size = int(n_frames / (stride1 * stride2))\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = [channels,\n",
    "                              channels * 8,\n",
    "                              channels * 4,\n",
    "                              channels * 8, channels * 8,\n",
    "                              channels * 16, channels * 16,\n",
    "                              channels * 32, channels * 32,\n",
    "                              channels * 64, channels * 64, n_class]\n",
    "\n",
    "        ch_confing_10 = 8 * 32  # 8 * 64\n",
    "        ch_n_class = n_class\n",
    "        conv1, bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1))\n",
    "        conv2, bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2))\n",
    "        conv3, bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1)\n",
    "        conv4, bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1)\n",
    "        conv5, bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1)\n",
    "        conv6, bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1)\n",
    "        conv7, bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1)\n",
    "        conv8, bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1)\n",
    "        conv9, bn9 = self.make_layers(ch_confing_10, ch_n_class, (1, 1))\n",
    "        fcn = nn.Linear(fcn_no_of_inputs, ch_n_class)\n",
    "        nn.init.kaiming_normal_(fcn.weight, nonlinearity='sigmoid')  # Kaiming with sigmoid is equivalent to LeCun normal in Keras\n",
    "\n",
    "        self.sfeb = nn.Sequential(\n",
    "            # Start: Filter bank\n",
    "            conv1, bn1, nn.ReLU(),\n",
    "            conv2, bn2, nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "        )\n",
    "\n",
    "        tfeb_modules = []\n",
    "        self.tfeb_width = int(((self.input_length / sr) * 1000) / 10)  # 10ms frames of audio length in seconds\n",
    "        tfeb_pool_sizes = self.get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width)\n",
    "        p_index = 0\n",
    "        for i in [3, 4, 6, 8]:  # ,10\n",
    "            tfeb_modules.extend([eval('conv{}'.format(i)), eval('bn{}'.format(i)), nn.ReLU()])\n",
    "\n",
    "            if i != 3:\n",
    "                tfeb_modules.extend([eval('conv{}'.format(i + 1)), eval('bn{}'.format(i + 1)), nn.ReLU()])\n",
    "\n",
    "            h, w = tfeb_pool_sizes[p_index]\n",
    "            if h > 1 or w > 1:\n",
    "                if i == 8:\n",
    "                    break\n",
    "                else:\n",
    "                    tfeb_modules.append(nn.MaxPool2d(kernel_size=(h, w)))\n",
    "            p_index += 1\n",
    "\n",
    "        tfeb_modules.append(nn.Dropout(0.2))\n",
    "        # tfeb_modules.extend([conv9, bn9, nn.ReLU()])\n",
    "        h, w = tfeb_pool_sizes[-1]\n",
    "        if h > 1 or w > 1:\n",
    "            tfeb_modules.append(nn.AvgPool2d(kernel_size=(5,7))) # h, w\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn])\n",
    "\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            \n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sfeb(x)\n",
    "        x = x.permute((0, 2, 1, 3))\n",
    "        x = self.tfeb(x)\n",
    "        y = self.output[0](x)\n",
    "        return y\n",
    "\n",
    "    def make_layers(self, in_channels, out_channels, kernel_size, stride=(1, 1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                          stride=stride, padding=padding, bias=bias)\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu')  # Kaiming with ReLU\n",
    "        bn = nn.BatchNorm2d(out_channels)\n",
    "        return conv, bn\n",
    "\n",
    "    def get_tfeb_pool_sizes(self, con2_ch, width):\n",
    "        h = self.get_tfeb_pool_size_component(con2_ch)\n",
    "        w = self.get_tfeb_pool_size_component(width)\n",
    "        pool_size = []\n",
    "        for (h1, w1) in zip(h, w):\n",
    "            pool_size.append((h1, w1))\n",
    "        return pool_size\n",
    "\n",
    "    def get_tfeb_pool_size_component(self, length):\n",
    "        c = []\n",
    "        index = 1\n",
    "        while index <= 6:\n",
    "            if length >= 2:\n",
    "                if index == 6:\n",
    "                    c.append(length)\n",
    "                else:\n",
    "                    c.append(2)\n",
    "                    length = length // 2\n",
    "            else:\n",
    "                c.append(1)\n",
    "            index += 1\n",
    "        return c\n",
    "\n",
    "def GetCustomedACDNetModel(input_len=inp_len, nclass=ch_n_class, sr=sr, channel_config=None):\n",
    "    net = Customed_ACDNetV2(input_len, nclass, sr, ch_conf=channel_config);\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580fd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sfeb.0.weight torch.Size([8, 1, 1, 9])\n",
      "sfeb.1.weight torch.Size([8])\n",
      "sfeb.1.bias torch.Size([8])\n",
      "sfeb.1.running_mean torch.Size([8])\n",
      "sfeb.1.running_var torch.Size([8])\n",
      "sfeb.1.num_batches_tracked torch.Size([])\n",
      "sfeb.3.weight torch.Size([64, 8, 1, 5])\n",
      "sfeb.4.weight torch.Size([64])\n",
      "sfeb.4.bias torch.Size([64])\n",
      "sfeb.4.running_mean torch.Size([64])\n",
      "sfeb.4.running_var torch.Size([64])\n",
      "sfeb.4.num_batches_tracked torch.Size([])\n",
      "tfeb.0.weight torch.Size([32, 1, 3, 3])\n",
      "tfeb.1.weight torch.Size([32])\n",
      "tfeb.1.bias torch.Size([32])\n",
      "tfeb.1.running_mean torch.Size([32])\n",
      "tfeb.1.running_var torch.Size([32])\n",
      "tfeb.1.num_batches_tracked torch.Size([])\n",
      "tfeb.4.weight torch.Size([64, 32, 3, 3])\n",
      "tfeb.5.weight torch.Size([64])\n",
      "tfeb.5.bias torch.Size([64])\n",
      "tfeb.5.running_mean torch.Size([64])\n",
      "tfeb.5.running_var torch.Size([64])\n",
      "tfeb.5.num_batches_tracked torch.Size([])\n",
      "tfeb.7.weight torch.Size([64, 64, 3, 3])\n",
      "tfeb.8.weight torch.Size([64])\n",
      "tfeb.8.bias torch.Size([64])\n",
      "tfeb.8.running_mean torch.Size([64])\n",
      "tfeb.8.running_var torch.Size([64])\n",
      "tfeb.8.num_batches_tracked torch.Size([])\n",
      "tfeb.11.weight torch.Size([128, 64, 3, 3])\n",
      "tfeb.12.weight torch.Size([128])\n",
      "tfeb.12.bias torch.Size([128])\n",
      "tfeb.12.running_mean torch.Size([128])\n",
      "tfeb.12.running_var torch.Size([128])\n",
      "tfeb.12.num_batches_tracked torch.Size([])\n",
      "tfeb.14.weight torch.Size([128, 128, 3, 3])\n",
      "tfeb.15.weight torch.Size([128])\n",
      "tfeb.15.bias torch.Size([128])\n",
      "tfeb.15.running_mean torch.Size([128])\n",
      "tfeb.15.running_var torch.Size([128])\n",
      "tfeb.15.num_batches_tracked torch.Size([])\n",
      "tfeb.18.weight torch.Size([256, 128, 3, 3])\n",
      "tfeb.19.weight torch.Size([256])\n",
      "tfeb.19.bias torch.Size([256])\n",
      "tfeb.19.running_mean torch.Size([256])\n",
      "tfeb.19.running_var torch.Size([256])\n",
      "tfeb.19.num_batches_tracked torch.Size([])\n",
      "tfeb.21.weight torch.Size([7, 256, 1, 1])\n",
      "tfeb.22.weight torch.Size([7])\n",
      "tfeb.22.bias torch.Size([7])\n",
      "tfeb.22.running_mean torch.Size([7])\n",
      "tfeb.22.running_var torch.Size([7])\n",
      "tfeb.22.num_batches_tracked torch.Size([])\n",
      "tfeb.27.weight torch.Size([7, 7])\n",
      "tfeb.27.bias torch.Size([7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1163137/312357234.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  models = torch.load(use_model_in_step2, map_location='cuda:0')['weight']\n"
     ]
    }
   ],
   "source": [
    "models = torch.load(use_model_in_step2, map_location='cuda:0')['weight']\n",
    "for i in models.keys():\n",
    "    print(i, models[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22423f68-d925-450f-a24b-567422ef736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='../datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    #Leqarning settings\n",
    "   \n",
    "    opt.batchSize = 32;\n",
    "    opt.LR = 0.05;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.weightDecay = 5e-4;\n",
    "    opt.schedule = [0.1,0.3,0.5,0.9];#default:[0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "    opt.warmup = 10;\n",
    "    opt.nEpochs = 1000;  ############33\n",
    "\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = ch_n_class#50;\n",
    "    opt.nFolds = 1;#5;\n",
    "    opt.split = 1#[i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = sr;\n",
    "    opt.inputLength = inp_len;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6e457-9c01-4aeb-bf23-6ae046b2d781",
   "metadata": {},
   "source": [
    "- <font size=2 color='#FF6600'>For the accuracy and model generation capacity it is better to add more data to the training and validation datasets.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48a4c5d5-e2b4-4775-b7f6-4de0c5147e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    \n",
    "    dataset = np.load(opt.Data_npz_path, allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "\n",
    "    train_sounds = dataset['sounds_train']\n",
    "    train_labels = dataset['labels_train']\n",
    "\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec8011c4-e99c-4675-b692-204ee0cfd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruningTrainer:\n",
    "    global logObj\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt;\n",
    "        #Conditional compression settings\n",
    "        # self.opt.LR = 0.01;\n",
    "        # self.opt.momentum = 0.09;\n",
    "        # self.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "        # self.opt.warmup = 0;\n",
    "        self.opt.prune_algo = 'l0norm';\n",
    "        self.opt.prune_interval = 1;\n",
    "        # self.opt.nEpochs = 1000;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.bestAcc = 0.0;\n",
    "        self.bestAccEpoch = 0;\n",
    "        self.trainGen = getTrainGen(opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        # if torch.device == \"cuda:0\":\n",
    "        #     self.device = '\"cuda:0\"'\n",
    "        # elif torch.device == \"mps\":\n",
    "        #     self.device = \"mps\"\n",
    "        # else:\n",
    "        #     self.device = \"cpu\"\n",
    "        # \n",
    "        # self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.opt.device = 'cuda:0'\n",
    "        else:\n",
    "            self.opt.device = 'cpu'\n",
    "        print(f\"!!! In PruningTrainer:: current used device:{self.opt.device}\")\n",
    "\n",
    "        self.start_time = time.time();\n",
    "\n",
    "    def PruneAndTrain(self):\n",
    "        self.trainGen;\n",
    "        print(self.opt.device);\n",
    "        loss_func = torch.nn.KLDivLoss(reduction='batchmean');\n",
    "\n",
    "        #Load saved model dict net.load_state_dict(torch.load(base_model_path, map_location=self.opt.device)['weight'], strict=False)\n",
    "        net = GetCustomedACDNetModel()#GetACDNetModel()\n",
    "        base_model_path = use_model_in_step1\n",
    "\n",
    "        net.load_state_dict(torch.load(base_model_path, map_location=self.opt.device)['weight'] ,strict=False);\n",
    "        calc.summary(net, (1,1,self.opt.inputLength))\n",
    "        net.eval();\n",
    "        val_acc, val_loss = self.__validate(net, loss_func);\n",
    "        print('Testing - Val: Loss {:.3f}  Acc(top1) {:.3f}%'.format(val_loss, val_acc));\n",
    "        net.train();\n",
    "\n",
    "        optimizer = optim.SGD(net.parameters(), lr=self.opt.LR, weight_decay=self.opt.weightDecay, momentum=self.opt.momentum, nesterov=True)\n",
    "\n",
    "        weight_name = [\"weight\"]# if not self.opt.factorize else [\"weightA\", \"weightB\", \"weightC\"]\n",
    "        layers_n = weight_pruner.layers_n(net, param_name=[\"weight\"])[1];\n",
    "        all_num = sum(layers_n.values());\n",
    "        print(\"\\t TOTAL PRUNABLE PARAMS: {}\".format(all_num));\n",
    "        print(\"\\t PRUNE RATIO :{}\".format(self.opt.prune_ratio));\n",
    "        sparse_factor = int(all_num * (1-self.opt.prune_ratio));\n",
    "        print(\"\\t SPARSE FACTOR: {}\".format(sparse_factor));\n",
    "        model_size = (sparse_factor * 4)/1024**2;\n",
    "        print(\"\\t MODEL SIZE: {:.2f} MB\".format(model_size));\n",
    "        prune_algo = getattr(weight_pruner, self.opt.prune_algo);\n",
    "        prune_func = lambda m: prune_algo(m, sparse_factor, param_name=weight_name);\n",
    "\n",
    "        for epoch_idx in range(self.opt.nEpochs):\n",
    "            epoch_start_time = time.time();\n",
    "            optimizer.param_groups[0]['lr'] = self.__get_lr(epoch_idx+1);\n",
    "            cur_lr = optimizer.param_groups[0]['lr'];\n",
    "            running_loss = 0.0;\n",
    "            running_acc = 0.0;\n",
    "            n_batches = math.ceil(len(self.trainGen.data)/self.opt.batchSize);\n",
    "            net.train();\n",
    "            for batch_idx in range(n_batches):\n",
    "                # with torch.no_grad():\n",
    "                x,y = self.trainGen.__getitem__(batch_idx)\n",
    "                x = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "                y = torch.tensor(y).to(self.opt.device);\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad();\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                # outputs = net(x);#in office and use cpu\n",
    "                x = x.type(torch.FloatTensor)\n",
    "                outputs = net(x);\n",
    "                res_y = y.argmax(dim=1)\n",
    "                res_y = res_y.type(torch.FloatTensor)\n",
    "                running_acc += ((( outputs.data.argmax(dim=1) == res_y)*1).float().mean()).item();\n",
    "                y = y.type(torch.FloatTensor)\n",
    "                loss = loss_func(outputs.log(), y);\n",
    "\n",
    "                loss.backward();\n",
    "                optimizer.step();\n",
    "\n",
    "                running_loss += loss.item();\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    prune_func(net);\n",
    "\n",
    "            prune_func(net)\n",
    "\n",
    "            tr_acc = (running_acc / n_batches)*100;\n",
    "            tr_loss = running_loss / n_batches;\n",
    "\n",
    "            #Epoch wise validation Validation\n",
    "            epoch_train_time = time.time() - epoch_start_time;\n",
    "            net.eval();\n",
    "            val_acc, val_loss = self.__validate(net, loss_func);\n",
    "            #Save best model\n",
    "            self.__save_model(val_acc, tr_acc, epoch_idx, net);\n",
    "\n",
    "            self.__on_epoch_end(epoch_start_time, epoch_train_time, epoch_idx, cur_lr, tr_loss, tr_acc, val_loss, val_acc);\n",
    "\n",
    "            running_loss = 0;\n",
    "            running_acc = 0;\n",
    "            net.train();\n",
    "\n",
    "        total_time_taken = time.time() - self.start_time;\n",
    "        print(\"Execution finished in: {}\".format(U.to_hms(total_time_taken)));\n",
    "\n",
    "    def load_test_data(self):\n",
    "        data = np.load(self.opt.Data_npz_path, allow_pickle=True);\n",
    "        print(f\"device is :{self.opt.device}\")\n",
    "        print(f\"len of Y:{len(data['labels_val'])}\")\n",
    "        dataX = data['sounds_val'].reshape(data['sounds_val'].shape[0],1,1,data['sounds_val'].shape[1]).astype(np.float32);\n",
    "        self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "        self.testY = torch.tensor(one_hot_encode(data['labels_val'], order)).type(torch.float32).to(self.opt.device);\n",
    "\n",
    "    def __get_lr(self, epoch):\n",
    "        divide_epoch = np.array([self.opt.nEpochs * i for i in self.opt.schedule]);\n",
    "        decay = sum(epoch > divide_epoch);\n",
    "        if epoch <= self.opt.warmup:\n",
    "            decay = 1;\n",
    "        return self.opt.LR * np.power(0.1, decay);\n",
    "\n",
    "    def __validate(self, net, lossFunc):\n",
    "        if self.testX is None:\n",
    "            self.load_test_data()\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops           \n",
    "            \n",
    "            for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "                x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "                x = torch.tensor(x)\n",
    "                x = x.type(torch.FloatTensor) # use apple mp2\n",
    "                scores = net(x);\n",
    "                y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "\n",
    "            acc, loss = self.__compute_accuracy(y_pred, self.testY, lossFunc);\n",
    "        return acc, loss;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy(self, y_pred, y_target, lossFunc):\n",
    "        with torch.no_grad():\n",
    "            #Reshape to shape theme like each sample comtains 10 samples, calculate mean and find theindices that has highest average value for each sample\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            # print(f\"y_pred:{type(y_pred)}, y_target:{type(y_target)}\")\n",
    "            y_target = y_target.cpu() #use apple m2, in office use cuda\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "            # valLossFunc = torch.nn.KLDivLoss();\n",
    "            loss = lossFunc(y_pred.float().log(), y_target.float()).item();\n",
    "            # loss = 0.0;\n",
    "        return acc, loss;\n",
    "\n",
    "    def __on_epoch_end(self, epoch_start_time, train_time, epochIdx, lr, tr_loss, tr_acc, val_loss, val_acc):\n",
    "        epoch_time = time.time() - epoch_start_time;\n",
    "        val_time = epoch_time - train_time;\n",
    "        total_time = time.time() - self.start_time;\n",
    "        line = '{} Epoch: {}/{} | Time: {} (Train {}  Val {}) | Train: LR {}  Loss {:.2f}  Acc {:.2f}% | Val: Loss {:.2f}  Acc(top1) {:.2f}% | HA {:.2f}@{}\\n'.format(\n",
    "            U.to_hms(total_time), epochIdx+1, self.opt.nEpochs, U.to_hms(epoch_time), U.to_hms(train_time), U.to_hms(val_time),\n",
    "            lr, tr_loss, tr_acc, val_loss, val_acc, self.bestAcc, self.bestAccEpoch);\n",
    "        # print(line)\n",
    "        sys.stdout.write(line);\n",
    "        sys.stdout.flush();\n",
    "\n",
    "    def __save_model(self, acc, train_acc, epochIdx, net):\n",
    "        if acc > self.bestAcc and acc > self.opt.first_save_acc:\n",
    "            self.bestAcc = acc;\n",
    "            self.bestAccEpoch = epochIdx +1;\n",
    "            self.__do_save_model(acc, train_acc, self.bestAccEpoch, net);\n",
    "        else:\n",
    "            if acc > self.opt.save_val_acc and train_acc > self.opt.save_train_acc: \n",
    "                self.__do_save_model(acc, train_acc, epochIdx, net);\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def __do_save_model(self, acc, tr_acc, epochIdx, net):\n",
    "        save_model_name = self.opt.model_name.format(self.bestAcc, acc, tr_acc, epochIdx, genDataTimeStr());\n",
    "        save_model_fullpath = self.opt.save_dir + save_model_name;\n",
    "        print(f\"save model to {save_model_fullpath}\")\n",
    "        torch.save({'weight':net.state_dict(), 'config':net.ch_config}, save_model_fullpath);\n",
    "        logObj.write(f\"save model:{self.opt.model_name}, bestAcc:{self.bestAcc}, valAcc:{acc}, trainAcc:{tr_acc}, record@{epochIdx}-epoch\");\n",
    "        logObj.write(\"\\n\");\n",
    "        logObj.flush();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9fd15f0-cbdf-4b2f-b3c9-96f694eb4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global logObj;\n",
    "    opt = getOpts()\n",
    "    opt.Data_npz_path = PATH\n",
    "    opt.sr = sr;\n",
    "    opt.inputLength = inp_len;\n",
    "    opt.trainer = None\n",
    "    opt.prune_ratio = 0.9; # for faster we try the bigger pruning ratio first!!! 8/28 \n",
    "    opt.first_save_acc = 65.0;\n",
    "    opt.save_val_acc = 70.0;\n",
    "    opt.save_train_acc = 85.0;\n",
    "    trainStartTime = getDateStr();\n",
    "    save_dir = f\"{save_pt_model_path}/pruning_4C_time_{trainStartTime}_prunratio{opt.prune_ratio*100}/\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    opt.save_dir = save_dir;\n",
    "    opt.model_name = \"uec_4Cmodel_first_stage_prun_haacc_{}_valacc{}_tracc{}_epoch_{}_{}.pt\";\n",
    "    print(\"Initializing PruneAndTrain Object.....\")\n",
    "    trainer = PruningTrainer(opt)#TLTrainer(opt)\n",
    "    print(\"Start to pruning.....\")\n",
    "    logSaveDir = \"./first_stage_pruning_logs/\"\n",
    "    ChkAndCreateSingleDir(logSaveDir);\n",
    "    logName = \"FirstPruningLog_{}.log\".format(trainStartTime);\n",
    "    logObj = open(os.path.join(logSaveDir,logName),'w');\n",
    "    trainer.PruneAndTrain();\n",
    "    logObj.flush();\n",
    "    logObj.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8780ef94-ad0c-46bc-abb9-a5573b8b3480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PruneAndTrain Object.....\n",
      "length of samples:1018\n",
      "!!! In PruningTrainer:: current used device:cuda:0\n",
      "Start to pruning.....\n",
      "cuda:0\n",
      "plt\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 20150)     (8, 1, 10071)         72      725,112\n",
      "  BatchNorm2d-2     (8, 1, 10071)     (8, 1, 10071)         16            0\n",
      "         ReLu-3     (8, 1, 10071)     (8, 1, 10071)          0       80,568\n",
      "       Conv2d-4     (8, 1, 10071)     (64, 1, 5034)      2,560   12,887,040\n",
      "  BatchNorm2d-5     (64, 1, 5034)     (64, 1, 5034)        128            0\n",
      "         ReLu-6     (64, 1, 5034)     (64, 1, 5034)          0      322,176\n",
      "    MaxPool2d-7     (64, 1, 5034)      (64, 1, 100)          0      320,000\n",
      "      Permute-8      (64, 1, 100)      (1, 64, 100)          0            0\n",
      "       Conv2d-9      (1, 64, 100)     (32, 64, 100)        288    1,843,200\n",
      " BatchNorm2d-10     (32, 64, 100)     (32, 64, 100)         64            0\n",
      "        ReLu-11     (32, 64, 100)     (32, 64, 100)          0      204,800\n",
      "   MaxPool2d-12     (32, 64, 100)      (32, 32, 50)          0      204,800\n",
      "      Conv2d-13      (32, 32, 50)      (64, 32, 50)     18,432   29,491,200\n",
      " BatchNorm2d-14      (64, 32, 50)      (64, 32, 50)        128            0\n",
      "        ReLu-15      (64, 32, 50)      (64, 32, 50)          0      102,400\n",
      "      Conv2d-16      (64, 32, 50)      (64, 32, 50)     36,864   58,982,400\n",
      " BatchNorm2d-17      (64, 32, 50)      (64, 32, 50)        128            0\n",
      "        ReLu-18      (64, 32, 50)      (64, 32, 50)          0      102,400\n",
      "   MaxPool2d-19      (64, 32, 50)      (64, 16, 25)          0      102,400\n",
      "      Conv2d-20      (64, 16, 25)     (128, 16, 25)     73,728   29,491,200\n",
      " BatchNorm2d-21     (128, 16, 25)     (128, 16, 25)        256            0\n",
      "        ReLu-22     (128, 16, 25)     (128, 16, 25)          0       51,200\n",
      "      Conv2d-23     (128, 16, 25)     (128, 16, 25)    147,456   58,982,400\n",
      " BatchNorm2d-24     (128, 16, 25)     (128, 16, 25)        256            0\n",
      "        ReLu-25     (128, 16, 25)     (128, 16, 25)          0       51,200\n",
      "   MaxPool2d-26     (128, 16, 25)      (128, 8, 12)          0       49,152\n",
      "      Conv2d-27      (128, 8, 12)      (256, 8, 12)    294,912   28,311,552\n",
      " BatchNorm2d-28      (256, 8, 12)      (256, 8, 12)        512            0\n",
      "        ReLu-29      (256, 8, 12)      (256, 8, 12)          0       24,576\n",
      "      Conv2d-30      (256, 8, 12)        (7, 8, 12)      1,792      172,032\n",
      " BatchNorm2d-31        (7, 8, 12)        (7, 8, 12)         14            0\n",
      "        ReLu-32        (7, 8, 12)        (7, 8, 12)          0          672\n",
      "   AvgPool2d-33        (7, 8, 12)         (7, 1, 1)          0          245\n",
      "     Flatten-34         (7, 1, 1)            (1, 7)          0            0\n",
      "      Linear-35            (1, 7)            (1, 7)         56           56\n",
      "     Softmax-36            (1, 7)            (1, 7)          0            7\n",
      "==============================================================================\n",
      "Total Params: 577,662\n",
      "Total FLOPs : 222,502,788\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.08\n",
      "Params size (MB): 2.20\n",
      "Total size (MB) : 2.28\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "plt_____\n",
      "device is :cuda:0\n",
      "len of Y:100\n",
      "Testing - Val: Loss nan  Acc(top1) 6.000%\n",
      "\t TOTAL PRUNABLE PARAMS: 576153\n",
      "\t PRUNE RATIO :0.9\n",
      "\t SPARSE FACTOR: 57615\n",
      "\t MODEL SIZE: 0.22 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1162272/194291979.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(base_model_path, map_location=self.opt.device)['weight'] ,strict=False);\n",
      "/tmp/ipykernel_1162272/194291979.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m08s Epoch: 1/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 1.49  Acc 46.39% | Val: Loss nan  Acc(top1) 12.00% | HA 0.00@0\n",
      "0m16s Epoch: 2/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 1.26  Acc 48.05% | Val: Loss nan  Acc(top1) 16.00% | HA 0.00@0\n",
      "0m25s Epoch: 3/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 1.13  Acc 56.25% | Val: Loss nan  Acc(top1) 22.00% | HA 0.00@0\n",
      "0m33s Epoch: 4/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 1.12  Acc 53.32% | Val: Loss nan  Acc(top1) 30.00% | HA 0.00@0\n",
      "0m42s Epoch: 5/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 1.05  Acc 53.03% | Val: Loss nan  Acc(top1) 30.00% | HA 0.00@0\n",
      "0m50s Epoch: 6/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 1.05  Acc 50.29% | Val: Loss nan  Acc(top1) 28.00% | HA 0.00@0\n",
      "0m58s Epoch: 7/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.99  Acc 53.61% | Val: Loss nan  Acc(top1) 32.00% | HA 0.00@0\n",
      "1m07s Epoch: 8/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.95  Acc 54.79% | Val: Loss nan  Acc(top1) 32.00% | HA 0.00@0\n",
      "1m15s Epoch: 9/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.97  Acc 53.12% | Val: Loss nan  Acc(top1) 32.00% | HA 0.00@0\n",
      "1m24s Epoch: 10/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.90  Acc 57.32% | Val: Loss nan  Acc(top1) 40.00% | HA 0.00@0\n",
      "1m32s Epoch: 11/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.86  Acc 56.84% | Val: Loss nan  Acc(top1) 42.00% | HA 0.00@0\n",
      "1m40s Epoch: 12/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.76  Acc 61.91% | Val: Loss nan  Acc(top1) 48.00% | HA 0.00@0\n",
      "1m49s Epoch: 13/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.73  Acc 63.87% | Val: Loss nan  Acc(top1) 46.00% | HA 0.00@0\n",
      "1m58s Epoch: 14/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.67  Acc 65.62% | Val: Loss nan  Acc(top1) 60.00% | HA 0.00@0\n",
      "2m06s Epoch: 15/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.64  Acc 64.65% | Val: Loss nan  Acc(top1) 62.00% | HA 0.00@0\n",
      "2m14s Epoch: 16/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.63  Acc 69.53% | Val: Loss nan  Acc(top1) 62.00% | HA 0.00@0\n",
      "2m23s Epoch: 17/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.62  Acc 67.68% | Val: Loss nan  Acc(top1) 64.00% | HA 0.00@0\n",
      "save model to /home/sail/sound_project/sound_ai_v2/STEP/step_2/save_pt_model_s2/20240829_14/pruning_4C_time_2024082914_prunratio90.0/uec_4Cmodel_first_stage_prun_haacc_76.0_valacc76.0_tracc66.89453125_epoch_18_20240829145852.pt\n",
      "2m32s Epoch: 18/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.61  Acc 66.89% | Val: Loss nan  Acc(top1) 76.00% | HA 76.00@18\n",
      "2m40s Epoch: 19/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.60  Acc 70.21% | Val: Loss nan  Acc(top1) 70.00% | HA 76.00@18\n",
      "2m49s Epoch: 20/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.59  Acc 66.89% | Val: Loss nan  Acc(top1) 70.00% | HA 76.00@18\n",
      "2m57s Epoch: 21/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.59  Acc 70.31% | Val: Loss nan  Acc(top1) 50.00% | HA 76.00@18\n",
      "3m06s Epoch: 22/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.56  Acc 68.55% | Val: Loss nan  Acc(top1) 64.00% | HA 76.00@18\n",
      "3m14s Epoch: 23/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.57  Acc 72.07% | Val: Loss nan  Acc(top1) 58.00% | HA 76.00@18\n",
      "3m23s Epoch: 24/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.55  Acc 72.46% | Val: Loss nan  Acc(top1) 72.00% | HA 76.00@18\n",
      "3m31s Epoch: 25/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.58  Acc 69.43% | Val: Loss nan  Acc(top1) 66.00% | HA 76.00@18\n",
      "3m40s Epoch: 26/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.56  Acc 71.09% | Val: Loss nan  Acc(top1) 74.00% | HA 76.00@18\n",
      "3m49s Epoch: 27/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.53  Acc 72.85% | Val: Loss nan  Acc(top1) 72.00% | HA 76.00@18\n",
      "3m57s Epoch: 28/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.55  Acc 69.92% | Val: Loss nan  Acc(top1) 60.00% | HA 76.00@18\n",
      "4m06s Epoch: 29/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.50  Acc 75.00% | Val: Loss nan  Acc(top1) 68.00% | HA 76.00@18\n",
      "4m14s Epoch: 30/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.51  Acc 71.19% | Val: Loss nan  Acc(top1) 66.00% | HA 76.00@18\n",
      "save model to /home/sail/sound_project/sound_ai_v2/STEP/step_2/save_pt_model_s2/20240829_14/pruning_4C_time_2024082914_prunratio90.0/uec_4Cmodel_first_stage_prun_haacc_78.0_valacc78.0_tracc71.77734375_epoch_31_20240829150043.pt\n",
      "4m23s Epoch: 31/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.52  Acc 71.78% | Val: Loss nan  Acc(top1) 78.00% | HA 78.00@31\n",
      "4m32s Epoch: 32/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.52  Acc 72.17% | Val: Loss nan  Acc(top1) 68.00% | HA 78.00@31\n",
      "4m40s Epoch: 33/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.52  Acc 73.54% | Val: Loss nan  Acc(top1) 74.00% | HA 78.00@31\n",
      "4m49s Epoch: 34/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.50  Acc 74.41% | Val: Loss nan  Acc(top1) 78.00% | HA 78.00@31\n",
      "4m57s Epoch: 35/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.51  Acc 75.10% | Val: Loss nan  Acc(top1) 66.00% | HA 78.00@31\n",
      "5m06s Epoch: 36/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.51  Acc 73.44% | Val: Loss nan  Acc(top1) 66.00% | HA 78.00@31\n",
      "5m14s Epoch: 37/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.48  Acc 77.44% | Val: Loss nan  Acc(top1) 74.00% | HA 78.00@31\n",
      "save model to /home/sail/sound_project/sound_ai_v2/STEP/step_2/save_pt_model_s2/20240829_14/pruning_4C_time_2024082914_prunratio90.0/uec_4Cmodel_first_stage_prun_haacc_82.0_valacc82.0_tracc75.9765625_epoch_38_20240829150143.pt\n",
      "5m23s Epoch: 38/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.46  Acc 75.98% | Val: Loss nan  Acc(top1) 82.00% | HA 82.00@38\n",
      "5m31s Epoch: 39/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.47  Acc 75.78% | Val: Loss nan  Acc(top1) 76.00% | HA 82.00@38\n",
      "5m40s Epoch: 40/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.48  Acc 75.59% | Val: Loss nan  Acc(top1) 70.00% | HA 82.00@38\n",
      "5m48s Epoch: 41/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.48  Acc 75.29% | Val: Loss nan  Acc(top1) 62.00% | HA 82.00@38\n",
      "5m57s Epoch: 42/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.50  Acc 74.61% | Val: Loss nan  Acc(top1) 80.00% | HA 82.00@38\n",
      "6m06s Epoch: 43/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.49  Acc 74.90% | Val: Loss nan  Acc(top1) 78.00% | HA 82.00@38\n",
      "6m14s Epoch: 44/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.48  Acc 75.68% | Val: Loss nan  Acc(top1) 68.00% | HA 82.00@38\n",
      "6m23s Epoch: 45/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.45  Acc 77.05% | Val: Loss nan  Acc(top1) 78.00% | HA 82.00@38\n",
      "6m31s Epoch: 46/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.46  Acc 77.54% | Val: Loss nan  Acc(top1) 74.00% | HA 82.00@38\n",
      "6m40s Epoch: 47/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.45  Acc 76.07% | Val: Loss nan  Acc(top1) 82.00% | HA 82.00@38\n",
      "6m48s Epoch: 48/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.48  Acc 75.98% | Val: Loss nan  Acc(top1) 70.00% | HA 82.00@38\n",
      "6m57s Epoch: 49/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.46  Acc 78.32% | Val: Loss nan  Acc(top1) 66.00% | HA 82.00@38\n",
      "7m06s Epoch: 50/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.46  Acc 77.25% | Val: Loss nan  Acc(top1) 78.00% | HA 82.00@38\n",
      "7m14s Epoch: 51/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.43  Acc 77.83% | Val: Loss nan  Acc(top1) 78.00% | HA 82.00@38\n",
      "7m23s Epoch: 52/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.48  Acc 75.59% | Val: Loss nan  Acc(top1) 72.00% | HA 82.00@38\n",
      "7m32s Epoch: 53/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.47  Acc 75.68% | Val: Loss nan  Acc(top1) 76.00% | HA 82.00@38\n",
      "7m40s Epoch: 54/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.46  Acc 75.39% | Val: Loss nan  Acc(top1) 74.00% | HA 82.00@38\n",
      "save model to /home/sail/sound_project/sound_ai_v2/STEP/step_2/save_pt_model_s2/20240829_14/pruning_4C_time_2024082914_prunratio90.0/uec_4Cmodel_first_stage_prun_haacc_86.0_valacc86.0_tracc75.9765625_epoch_55_20240829150409.pt\n",
      "7m49s Epoch: 55/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.45  Acc 75.98% | Val: Loss nan  Acc(top1) 86.00% | HA 86.00@55\n",
      "7m57s Epoch: 56/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.47  Acc 78.22% | Val: Loss nan  Acc(top1) 78.00% | HA 86.00@55\n",
      "8m06s Epoch: 57/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.46  Acc 77.83% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "8m14s Epoch: 58/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.45  Acc 76.95% | Val: Loss nan  Acc(top1) 80.00% | HA 86.00@55\n",
      "8m23s Epoch: 59/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.45  Acc 75.68% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "8m31s Epoch: 60/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.42  Acc 78.71% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "8m39s Epoch: 61/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.46  Acc 76.27% | Val: Loss nan  Acc(top1) 86.00% | HA 86.00@55\n",
      "8m48s Epoch: 62/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.45  Acc 76.86% | Val: Loss nan  Acc(top1) 58.00% | HA 86.00@55\n",
      "8m56s Epoch: 63/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.46  Acc 77.54% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "9m05s Epoch: 64/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.45  Acc 77.15% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "9m13s Epoch: 65/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.42  Acc 79.59% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "9m22s Epoch: 66/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.49  Acc 76.46% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "9m30s Epoch: 67/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.46  Acc 78.61% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "9m39s Epoch: 68/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.43  Acc 77.34% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "9m47s Epoch: 69/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.45  Acc 78.22% | Val: Loss nan  Acc(top1) 78.00% | HA 86.00@55\n",
      "9m55s Epoch: 70/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.46  Acc 76.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "10m04s Epoch: 71/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.43  Acc 79.88% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "10m12s Epoch: 72/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.44  Acc 76.56% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "10m21s Epoch: 73/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.45  Acc 77.25% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "10m29s Epoch: 74/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.43  Acc 76.27% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "10m37s Epoch: 75/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.44  Acc 75.98% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "10m46s Epoch: 76/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.47  Acc 76.66% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "10m55s Epoch: 77/1000 | Time: 0m09s (Train 0m09s  Val 0m00s) | Train: LR 0.05  Loss 0.42  Acc 77.64% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "11m04s Epoch: 78/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.41  Acc 78.81% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "11m12s Epoch: 79/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.41  Acc 78.61% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "11m21s Epoch: 80/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.44  Acc 76.17% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "11m29s Epoch: 81/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.43  Acc 76.37% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "11m37s Epoch: 82/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.43  Acc 77.54% | Val: Loss nan  Acc(top1) 78.00% | HA 86.00@55\n",
      "11m46s Epoch: 83/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.42  Acc 76.95% | Val: Loss nan  Acc(top1) 60.00% | HA 86.00@55\n",
      "11m54s Epoch: 84/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.40  Acc 78.42% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "12m03s Epoch: 85/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.42  Acc 76.46% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "12m11s Epoch: 86/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.46  Acc 76.95% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "12m20s Epoch: 87/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.45  Acc 77.34% | Val: Loss nan  Acc(top1) 78.00% | HA 86.00@55\n",
      "12m28s Epoch: 88/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.45  Acc 77.05% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "12m36s Epoch: 89/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.43  Acc 78.03% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "12m45s Epoch: 90/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.42  Acc 76.95% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "12m54s Epoch: 91/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.44  Acc 78.52% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "13m02s Epoch: 92/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.43  Acc 77.64% | Val: Loss nan  Acc(top1) 80.00% | HA 86.00@55\n",
      "13m11s Epoch: 93/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.43  Acc 77.83% | Val: Loss nan  Acc(top1) 54.00% | HA 86.00@55\n",
      "13m20s Epoch: 94/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.43  Acc 81.05% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "13m28s Epoch: 95/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.41  Acc 75.98% | Val: Loss nan  Acc(top1) 78.00% | HA 86.00@55\n",
      "13m36s Epoch: 96/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.41  Acc 79.30% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "13m45s Epoch: 97/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.43  Acc 77.44% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "13m53s Epoch: 98/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.39  Acc 81.84% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "14m02s Epoch: 99/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.42  Acc 77.93% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "14m10s Epoch: 100/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.05  Loss 0.42  Acc 78.71% | Val: Loss nan  Acc(top1) 58.00% | HA 86.00@55\n",
      "14m19s Epoch: 101/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.43  Acc 78.42% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "14m28s Epoch: 102/1000 | Time: 0m09s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.43  Acc 75.59% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "14m36s Epoch: 103/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.42  Acc 77.44% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "14m45s Epoch: 104/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.42  Acc 77.64% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "14m53s Epoch: 105/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.41  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "15m02s Epoch: 106/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.42  Acc 77.15% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "15m11s Epoch: 107/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.41  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "15m19s Epoch: 108/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 78.12% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "15m28s Epoch: 109/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 78.22% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "15m36s Epoch: 110/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.49% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "15m45s Epoch: 111/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.10% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "15m54s Epoch: 112/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.08% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "16m03s Epoch: 113/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 77.64% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "16m12s Epoch: 114/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.39% | Val: Loss nan  Acc(top1) 62.00% | HA 86.00@55\n",
      "16m21s Epoch: 115/1000 | Time: 0m09s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 79.49% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "16m29s Epoch: 116/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.41  Acc 77.34% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "16m38s Epoch: 117/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 76.37% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "16m47s Epoch: 118/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 78.52% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "16m55s Epoch: 119/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.39% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "17m04s Epoch: 120/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 80.18% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "17m12s Epoch: 121/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 80.57% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "17m21s Epoch: 122/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.88% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "17m29s Epoch: 123/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 77.54% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "17m38s Epoch: 124/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.20% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "17m47s Epoch: 125/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 81.35% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "17m55s Epoch: 126/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 77.54% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "18m03s Epoch: 127/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 79.00% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "18m12s Epoch: 128/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 81.93% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "18m20s Epoch: 129/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 80.08% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "18m29s Epoch: 130/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 82.52% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "18m37s Epoch: 131/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.59% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "18m45s Epoch: 132/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.41  Acc 79.98% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "18m54s Epoch: 133/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 79.98% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "19m02s Epoch: 134/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 77.73% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "19m10s Epoch: 135/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 79.39% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "19m19s Epoch: 136/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 78.81% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "19m27s Epoch: 137/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 81.84% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "19m35s Epoch: 138/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.49% | Val: Loss nan  Acc(top1) 78.00% | HA 86.00@55\n",
      "19m44s Epoch: 139/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 77.44% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "19m52s Epoch: 140/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 78.32% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "20m01s Epoch: 141/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.49% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "20m09s Epoch: 142/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 82.13% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "20m18s Epoch: 143/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 78.91% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "20m27s Epoch: 144/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 77.25% | Val: Loss nan  Acc(top1) 78.00% | HA 86.00@55\n",
      "20m35s Epoch: 145/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.10% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "20m44s Epoch: 146/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "20m52s Epoch: 147/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 80.47% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "21m01s Epoch: 148/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 80.57% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "21m10s Epoch: 149/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 78.91% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "21m18s Epoch: 150/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.45% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "21m27s Epoch: 151/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 80.47% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "21m35s Epoch: 152/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 82.03% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "21m44s Epoch: 153/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 80.08% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "21m53s Epoch: 154/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.69% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "22m01s Epoch: 155/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.69% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "22m10s Epoch: 156/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 78.42% | Val: Loss nan  Acc(top1) 62.00% | HA 86.00@55\n",
      "22m18s Epoch: 157/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 77.83% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "22m27s Epoch: 158/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "22m36s Epoch: 159/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.10% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "22m44s Epoch: 160/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.98% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "22m53s Epoch: 161/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 77.05% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "23m02s Epoch: 162/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "23m10s Epoch: 163/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 81.05% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "23m19s Epoch: 164/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.69% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "23m27s Epoch: 165/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.49% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "23m36s Epoch: 166/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 80.76% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "23m45s Epoch: 167/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.49% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "23m54s Epoch: 168/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.49% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "24m02s Epoch: 169/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.98% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "24m11s Epoch: 170/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.84% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "24m19s Epoch: 171/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "24m28s Epoch: 172/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.59% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "24m37s Epoch: 173/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 81.25% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "24m46s Epoch: 174/1000 | Time: 0m09s (Train 0m09s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.10% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "24m55s Epoch: 175/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.47% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "25m04s Epoch: 176/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 81.15% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "25m12s Epoch: 177/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "25m21s Epoch: 178/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 81.15% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "25m29s Epoch: 179/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 80.86% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "25m38s Epoch: 180/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 78.03% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "25m46s Epoch: 181/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.88% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "25m55s Epoch: 182/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 81.05% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "26m03s Epoch: 183/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 78.71% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "26m12s Epoch: 184/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 80.08% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "26m20s Epoch: 185/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.30% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "26m29s Epoch: 186/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 78.52% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "26m37s Epoch: 187/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.96% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "26m46s Epoch: 188/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 78.61% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "26m54s Epoch: 189/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.47% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "27m03s Epoch: 190/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.10% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "27m11s Epoch: 191/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "27m20s Epoch: 192/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 81.35% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "27m29s Epoch: 193/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 78.52% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "27m37s Epoch: 194/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.59% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "27m46s Epoch: 195/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 83.01% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "27m54s Epoch: 196/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 81.74% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "28m02s Epoch: 197/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.98% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "28m11s Epoch: 198/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 78.12% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "28m19s Epoch: 199/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 82.42% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "28m28s Epoch: 200/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 77.54% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "28m36s Epoch: 201/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 81.05% | Val: Loss nan  Acc(top1) 78.00% | HA 86.00@55\n",
      "28m45s Epoch: 202/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 82.71% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "28m53s Epoch: 203/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.27% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "29m02s Epoch: 204/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.76% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "29m10s Epoch: 205/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 82.13% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "29m19s Epoch: 206/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 80.08% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "29m28s Epoch: 207/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.30% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "29m36s Epoch: 208/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 78.61% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "29m45s Epoch: 209/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.27% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "29m53s Epoch: 210/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.27% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "30m02s Epoch: 211/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 80.27% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "30m10s Epoch: 212/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 81.15% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "30m19s Epoch: 213/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 81.45% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "30m27s Epoch: 214/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "30m36s Epoch: 215/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 78.32% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "30m44s Epoch: 216/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.25% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "30m52s Epoch: 217/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.18% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "31m01s Epoch: 218/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 80.47% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "31m09s Epoch: 219/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 78.61% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "31m17s Epoch: 220/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "31m26s Epoch: 221/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "31m35s Epoch: 222/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 82.23% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "31m43s Epoch: 223/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 77.64% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "31m52s Epoch: 224/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.25% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "32m00s Epoch: 225/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 81.25% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "32m09s Epoch: 226/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.59% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "32m17s Epoch: 227/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.08% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "32m26s Epoch: 228/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 81.35% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "32m34s Epoch: 229/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.57% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "32m42s Epoch: 230/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.47% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "32m51s Epoch: 231/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.84% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "32m59s Epoch: 232/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.69% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "33m08s Epoch: 233/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.57% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "33m16s Epoch: 234/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 80.37% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "33m25s Epoch: 235/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.57% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "33m34s Epoch: 236/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 80.66% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "33m42s Epoch: 237/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.37% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "33m51s Epoch: 238/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 82.62% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "33m59s Epoch: 239/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 79.98% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "34m08s Epoch: 240/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.49% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "34m16s Epoch: 241/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 81.25% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "34m25s Epoch: 242/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 81.15% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "34m33s Epoch: 243/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.10% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "34m41s Epoch: 244/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.88% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "34m50s Epoch: 245/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 78.61% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "34m58s Epoch: 246/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.34  Acc 81.25% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "35m07s Epoch: 247/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 81.15% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "35m15s Epoch: 248/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.30% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "35m24s Epoch: 249/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 81.15% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "35m32s Epoch: 250/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.27% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "35m40s Epoch: 251/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 79.00% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "35m49s Epoch: 252/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.76% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "35m57s Epoch: 253/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.08% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "36m05s Epoch: 254/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 78.81% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "36m14s Epoch: 255/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 79.20% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "36m22s Epoch: 256/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "36m31s Epoch: 257/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 78.32% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "36m39s Epoch: 258/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.76% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "36m47s Epoch: 259/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.66% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "36m56s Epoch: 260/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 80.08% | Val: Loss nan  Acc(top1) 62.00% | HA 86.00@55\n",
      "37m04s Epoch: 261/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 82.03% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "37m13s Epoch: 262/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.10% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "37m21s Epoch: 263/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.35% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "37m30s Epoch: 264/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 80.57% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "37m39s Epoch: 265/1000 | Time: 0m09s (Train 0m09s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "37m47s Epoch: 266/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "37m56s Epoch: 267/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.08% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "38m05s Epoch: 268/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 81.45% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "38m13s Epoch: 269/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.15% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "38m22s Epoch: 270/1000 | Time: 0m09s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 79.39% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "38m31s Epoch: 271/1000 | Time: 0m09s (Train 0m09s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.30% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "38m40s Epoch: 272/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.38  Acc 80.47% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "38m48s Epoch: 273/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.47% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "38m57s Epoch: 274/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 81.93% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "39m05s Epoch: 275/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.98% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "39m14s Epoch: 276/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 79.88% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "39m22s Epoch: 277/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.47% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "39m30s Epoch: 278/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "39m38s Epoch: 279/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 78.71% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "39m47s Epoch: 280/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 79.69% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "39m55s Epoch: 281/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 79.20% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "40m03s Epoch: 282/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.35  Acc 80.37% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "40m12s Epoch: 283/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.39  Acc 78.22% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "40m20s Epoch: 284/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "40m28s Epoch: 285/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "40m36s Epoch: 286/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "40m45s Epoch: 287/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 82.62% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "40m53s Epoch: 288/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.40  Acc 80.08% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "41m01s Epoch: 289/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 78.91% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "41m10s Epoch: 290/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.93% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "41m18s Epoch: 291/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.98% | Val: Loss nan  Acc(top1) 78.00% | HA 86.00@55\n",
      "41m26s Epoch: 292/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "41m35s Epoch: 293/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.74% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "41m43s Epoch: 294/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 80.08% | Val: Loss nan  Acc(top1) 62.00% | HA 86.00@55\n",
      "41m51s Epoch: 295/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 78.42% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "42m00s Epoch: 296/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "42m09s Epoch: 297/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "42m17s Epoch: 298/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "42m26s Epoch: 299/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.36  Acc 80.47% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "42m34s Epoch: 300/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.005000000000000001  Loss 0.37  Acc 79.59% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "42m42s Epoch: 301/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.27% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "42m51s Epoch: 302/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.37% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "43m00s Epoch: 303/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.00% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "43m08s Epoch: 304/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 81.64% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "43m17s Epoch: 305/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.45% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "43m25s Epoch: 306/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.47% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "43m33s Epoch: 307/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 80.57% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "43m42s Epoch: 308/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.74% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "43m50s Epoch: 309/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.98% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "43m59s Epoch: 310/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 80.86% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "44m07s Epoch: 311/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 80.08% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "44m16s Epoch: 312/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 81.54% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "44m24s Epoch: 313/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.00% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "44m33s Epoch: 314/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.40  Acc 80.08% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "44m41s Epoch: 315/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.54% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "44m50s Epoch: 316/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 80.86% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "44m58s Epoch: 317/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.98% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "45m07s Epoch: 318/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.74% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "45m15s Epoch: 319/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 81.05% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "45m23s Epoch: 320/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 80.47% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "45m32s Epoch: 321/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.34  Acc 82.91% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "45m40s Epoch: 322/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.15% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "45m49s Epoch: 323/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 82.32% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "45m57s Epoch: 324/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 83.11% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "46m05s Epoch: 325/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 77.83% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "46m14s Epoch: 326/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.47% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "46m22s Epoch: 327/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.57% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "46m31s Epoch: 328/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 80.66% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "46m39s Epoch: 329/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.34  Acc 82.42% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "46m47s Epoch: 330/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.84% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "46m56s Epoch: 331/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.76% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "47m04s Epoch: 332/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 79.39% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "47m12s Epoch: 333/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.98% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "47m21s Epoch: 334/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.93% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "47m29s Epoch: 335/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.84% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "47m38s Epoch: 336/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.40  Acc 80.27% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "47m46s Epoch: 337/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 80.76% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "47m55s Epoch: 338/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.39  Acc 80.86% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "48m03s Epoch: 339/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.05% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "48m11s Epoch: 340/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 78.71% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "48m20s Epoch: 341/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.08% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "48m28s Epoch: 342/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.96% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "48m36s Epoch: 343/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.27% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "48m45s Epoch: 344/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.47% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "48m53s Epoch: 345/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 78.71% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "49m02s Epoch: 346/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 78.61% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "49m11s Epoch: 347/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.33  Acc 82.03% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "49m19s Epoch: 348/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.35% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "49m28s Epoch: 349/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "49m36s Epoch: 350/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.74% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "49m45s Epoch: 351/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.74% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "49m54s Epoch: 352/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 78.81% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "50m02s Epoch: 353/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.96% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "50m11s Epoch: 354/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.47% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "50m19s Epoch: 355/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.27% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "50m28s Epoch: 356/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 82.03% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "50m37s Epoch: 357/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.45% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "50m46s Epoch: 358/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.35% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "50m54s Epoch: 359/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.49% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "51m03s Epoch: 360/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 81.64% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "51m11s Epoch: 361/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.39  Acc 80.08% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "51m20s Epoch: 362/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 81.15% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "51m28s Epoch: 363/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 79.39% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "51m36s Epoch: 364/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.08% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "51m45s Epoch: 365/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.79% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "51m53s Epoch: 366/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 77.83% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "52m02s Epoch: 367/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.08% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "52m10s Epoch: 368/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.98% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "52m19s Epoch: 369/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 78.81% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "52m27s Epoch: 370/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 77.93% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "52m36s Epoch: 371/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "52m44s Epoch: 372/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 83.69% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "52m52s Epoch: 373/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.39  Acc 79.00% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "53m01s Epoch: 374/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.96% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "53m09s Epoch: 375/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.27% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "53m18s Epoch: 376/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.25% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "53m26s Epoch: 377/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.76% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "53m35s Epoch: 378/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 79.49% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "53m43s Epoch: 379/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.15% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "53m52s Epoch: 380/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.84% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "54m01s Epoch: 381/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.15% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "54m09s Epoch: 382/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.20% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "54m18s Epoch: 383/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.39% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "54m27s Epoch: 384/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 79.39% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "54m36s Epoch: 385/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 80.76% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "54m44s Epoch: 386/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.57% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "54m53s Epoch: 387/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "55m02s Epoch: 388/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "55m10s Epoch: 389/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 79.49% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "55m19s Epoch: 390/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "55m27s Epoch: 391/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 81.93% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "55m36s Epoch: 392/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 82.23% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "55m44s Epoch: 393/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 83.69% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "55m52s Epoch: 394/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.57% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "56m01s Epoch: 395/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 79.69% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "56m09s Epoch: 396/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.45% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "56m18s Epoch: 397/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 82.62% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "56m26s Epoch: 398/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "56m35s Epoch: 399/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.93% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "56m44s Epoch: 400/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.20% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "56m52s Epoch: 401/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 79.79% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "57m00s Epoch: 402/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 81.35% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "57m09s Epoch: 403/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "57m18s Epoch: 404/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 79.88% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "57m26s Epoch: 405/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.69% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "57m34s Epoch: 406/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "57m43s Epoch: 407/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.08% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "57m51s Epoch: 408/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.88% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "58m00s Epoch: 409/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "58m08s Epoch: 410/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.08% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "58m17s Epoch: 411/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "58m25s Epoch: 412/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 82.32% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "58m34s Epoch: 413/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.08% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "58m42s Epoch: 414/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.64% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "58m51s Epoch: 415/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 80.08% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "58m59s Epoch: 416/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "59m07s Epoch: 417/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.84% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "59m16s Epoch: 418/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "59m24s Epoch: 419/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 79.88% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "59m32s Epoch: 420/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.25% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "59m41s Epoch: 421/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 81.84% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "59m49s Epoch: 422/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.88% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "59m58s Epoch: 423/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.39  Acc 79.88% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "1h00m Epoch: 424/1000 | Time: 0m09s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 80.08% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h00m Epoch: 425/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h00m Epoch: 426/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.33  Acc 80.18% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h00m Epoch: 427/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 79.20% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h00m Epoch: 428/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 82.62% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h00m Epoch: 429/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.49% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "1h00m Epoch: 430/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.27% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h01m Epoch: 431/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 81.54% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h01m Epoch: 432/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 78.71% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h01m Epoch: 433/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.74% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h01m Epoch: 434/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.39  Acc 79.98% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h01m Epoch: 435/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h01m Epoch: 436/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 80.86% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h01m Epoch: 437/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 81.64% | Val: Loss nan  Acc(top1) 60.00% | HA 86.00@55\n",
      "1h02m Epoch: 438/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 83.01% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h02m Epoch: 439/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 77.64% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h02m Epoch: 440/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.25% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h02m Epoch: 441/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.98% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h02m Epoch: 442/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 78.81% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h02m Epoch: 443/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.33  Acc 80.96% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h02m Epoch: 444/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h03m Epoch: 445/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h03m Epoch: 446/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 79.39% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h03m Epoch: 447/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 78.52% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h03m Epoch: 448/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.88% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h03m Epoch: 449/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 80.96% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "1h03m Epoch: 450/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 78.52% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h03m Epoch: 451/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.33  Acc 82.62% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h04m Epoch: 452/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.96% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h04m Epoch: 453/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 77.93% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h04m Epoch: 454/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 81.05% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h04m Epoch: 455/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.34  Acc 79.88% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h04m Epoch: 456/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 78.91% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h04m Epoch: 457/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 80.47% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h04m Epoch: 458/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.39  Acc 78.42% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h05m Epoch: 459/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.34  Acc 82.62% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h05m Epoch: 460/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.34  Acc 82.03% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h05m Epoch: 461/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 80.47% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h05m Epoch: 462/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 80.76% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h05m Epoch: 463/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 81.05% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h05m Epoch: 464/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 77.83% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h05m Epoch: 465/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 80.66% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h06m Epoch: 466/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.49% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h06m Epoch: 467/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.93% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h06m Epoch: 468/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 78.81% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h06m Epoch: 469/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.59% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h06m Epoch: 470/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 79.49% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h06m Epoch: 471/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.64% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h06m Epoch: 472/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.25% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h07m Epoch: 473/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 79.00% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h07m Epoch: 474/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.35% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h07m Epoch: 475/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h07m Epoch: 476/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 83.50% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h07m Epoch: 477/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.18% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h07m Epoch: 478/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 78.61% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h07m Epoch: 479/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.39  Acc 79.49% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h08m Epoch: 480/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 79.98% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h08m Epoch: 481/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.66% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h08m Epoch: 482/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.34  Acc 82.13% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h08m Epoch: 483/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h08m Epoch: 484/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h08m Epoch: 485/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h08m Epoch: 486/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.49% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h09m Epoch: 487/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.39  Acc 78.52% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h09m Epoch: 488/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 80.96% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h09m Epoch: 489/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 79.88% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h09m Epoch: 490/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.39  Acc 79.59% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h09m Epoch: 491/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.39  Acc 77.73% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h09m Epoch: 492/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 79.88% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h09m Epoch: 493/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 82.23% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h10m Epoch: 494/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.36  Acc 81.25% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h10m Epoch: 495/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.35  Acc 81.45% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h10m Epoch: 496/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.40  Acc 79.10% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h10m Epoch: 497/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.34  Acc 82.03% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h10m Epoch: 498/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.33  Acc 81.54% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h10m Epoch: 499/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.38  Acc 81.25% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h10m Epoch: 500/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 0.0005000000000000001  Loss 0.39  Acc 78.52% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h11m Epoch: 501/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 79.59% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h11m Epoch: 502/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.27% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h11m Epoch: 503/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 82.13% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h11m Epoch: 504/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 80.37% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h11m Epoch: 505/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.74% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h11m Epoch: 506/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.27% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h11m Epoch: 507/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.00% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h12m Epoch: 508/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.54% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h12m Epoch: 509/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h12m Epoch: 510/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.10% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h12m Epoch: 511/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 78.71% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h12m Epoch: 512/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.57% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h12m Epoch: 513/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.30% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h12m Epoch: 514/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.15% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h13m Epoch: 515/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.98% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h13m Epoch: 516/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h13m Epoch: 517/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.76% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h13m Epoch: 518/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.08% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h13m Epoch: 519/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 81.54% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h13m Epoch: 520/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.27% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h13m Epoch: 521/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h14m Epoch: 522/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.35% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h14m Epoch: 523/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.49% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h14m Epoch: 524/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.49% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h14m Epoch: 525/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.39  Acc 80.08% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h14m Epoch: 526/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.00% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h14m Epoch: 527/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 77.93% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h14m Epoch: 528/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.45% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h15m Epoch: 529/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 78.91% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h15m Epoch: 530/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 82.03% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h15m Epoch: 531/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 79.59% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h15m Epoch: 532/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.76% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h15m Epoch: 533/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.27% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h15m Epoch: 534/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.30% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h15m Epoch: 535/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h16m Epoch: 536/1000 | Time: 0m09s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.10% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h16m Epoch: 537/1000 | Time: 0m09s (Train 0m09s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.18% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h16m Epoch: 538/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.03% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h16m Epoch: 539/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 83.11% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h16m Epoch: 540/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h16m Epoch: 541/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h16m Epoch: 542/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 81.05% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h17m Epoch: 543/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.96% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h17m Epoch: 544/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.00% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h17m Epoch: 545/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.05% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h17m Epoch: 546/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h17m Epoch: 547/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h17m Epoch: 548/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.74% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h17m Epoch: 549/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h18m Epoch: 550/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h18m Epoch: 551/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.25% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h18m Epoch: 552/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h18m Epoch: 553/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.42% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h18m Epoch: 554/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.30% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "1h18m Epoch: 555/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 81.54% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h18m Epoch: 556/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.15% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h19m Epoch: 557/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.69% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h19m Epoch: 558/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 82.81% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h19m Epoch: 559/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.54% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h19m Epoch: 560/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.66% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h19m Epoch: 561/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h19m Epoch: 562/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "1h19m Epoch: 563/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 83.01% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h20m Epoch: 564/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.03% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h20m Epoch: 565/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.37% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h20m Epoch: 566/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.15% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h20m Epoch: 567/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.59% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h20m Epoch: 568/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 78.61% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h20m Epoch: 569/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 80.08% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h20m Epoch: 570/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.49% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h21m Epoch: 571/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.27% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h21m Epoch: 572/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h21m Epoch: 573/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.37% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h21m Epoch: 574/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h21m Epoch: 575/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.64% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h21m Epoch: 576/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.00% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h21m Epoch: 577/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.45% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h22m Epoch: 578/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.98% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h22m Epoch: 579/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.45% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "1h22m Epoch: 580/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.13% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h22m Epoch: 581/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.66% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h22m Epoch: 582/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 83.20% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h22m Epoch: 583/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.39  Acc 77.83% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h22m Epoch: 584/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 82.13% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h23m Epoch: 585/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h23m Epoch: 586/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.57% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h23m Epoch: 587/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.35% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h23m Epoch: 588/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.57% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h23m Epoch: 589/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 80.66% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h23m Epoch: 590/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.79% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h23m Epoch: 591/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.96% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h24m Epoch: 592/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 78.42% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "1h24m Epoch: 593/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.69% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h24m Epoch: 594/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.15% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h24m Epoch: 595/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.35% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h24m Epoch: 596/1000 | Time: 0m09s (Train 0m09s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.57% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h24m Epoch: 597/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 82.23% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h24m Epoch: 598/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h25m Epoch: 599/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 82.42% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h25m Epoch: 600/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.88% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h25m Epoch: 601/1000 | Time: 0m09s (Train 0m09s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.93% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h25m Epoch: 602/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 80.27% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h25m Epoch: 603/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 78.52% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h25m Epoch: 604/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.40  Acc 78.52% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h25m Epoch: 605/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h26m Epoch: 606/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.35% | Val: Loss nan  Acc(top1) 58.00% | HA 86.00@55\n",
      "1h26m Epoch: 607/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.96% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h26m Epoch: 608/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.96% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "1h26m Epoch: 609/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 80.27% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h26m Epoch: 610/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.66% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h26m Epoch: 611/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h26m Epoch: 612/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.88% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h27m Epoch: 613/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h27m Epoch: 614/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.27% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h27m Epoch: 615/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h27m Epoch: 616/1000 | Time: 0m09s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h27m Epoch: 617/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 82.52% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h27m Epoch: 618/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 84.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h27m Epoch: 619/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.30% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h28m Epoch: 620/1000 | Time: 0m09s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.27% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h28m Epoch: 621/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 81.45% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h28m Epoch: 622/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 81.64% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h28m Epoch: 623/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h28m Epoch: 624/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.96% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h28m Epoch: 625/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 84.77% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h28m Epoch: 626/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.25% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h29m Epoch: 627/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.96% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h29m Epoch: 628/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.25% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h29m Epoch: 629/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.76% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h29m Epoch: 630/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 80.08% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h29m Epoch: 631/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h29m Epoch: 632/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 78.61% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h29m Epoch: 633/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 82.13% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h30m Epoch: 634/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "1h30m Epoch: 635/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.84% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h30m Epoch: 636/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 78.42% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h30m Epoch: 637/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.57% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h30m Epoch: 638/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 78.91% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h30m Epoch: 639/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.47% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h30m Epoch: 640/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.88% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h31m Epoch: 641/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 80.27% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h31m Epoch: 642/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 78.32% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h31m Epoch: 643/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 80.27% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "1h31m Epoch: 644/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.66% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h31m Epoch: 645/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 79.79% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h31m Epoch: 646/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 78.22% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h31m Epoch: 647/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.76% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h32m Epoch: 648/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.66% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h32m Epoch: 649/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h32m Epoch: 650/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.13% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h32m Epoch: 651/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.30% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h32m Epoch: 652/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.54% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h32m Epoch: 653/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 80.57% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h32m Epoch: 654/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.69% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h33m Epoch: 655/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.39% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h33m Epoch: 656/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.71% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h33m Epoch: 657/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.39  Acc 79.30% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h33m Epoch: 658/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.39  Acc 77.73% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h33m Epoch: 659/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.59% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h33m Epoch: 660/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h33m Epoch: 661/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 80.57% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h34m Epoch: 662/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.76% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h34m Epoch: 663/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.59% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h34m Epoch: 664/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 82.13% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h34m Epoch: 665/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 82.32% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h34m Epoch: 666/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.91% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h34m Epoch: 667/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 78.61% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h34m Epoch: 668/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.47% | Val: Loss nan  Acc(top1) 62.00% | HA 86.00@55\n",
      "1h35m Epoch: 669/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.18% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h35m Epoch: 670/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.47% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h35m Epoch: 671/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 79.79% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h35m Epoch: 672/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 81.45% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h35m Epoch: 673/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 78.91% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h35m Epoch: 674/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h35m Epoch: 675/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 83.11% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h36m Epoch: 676/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h36m Epoch: 677/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.88% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h36m Epoch: 678/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.64% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h36m Epoch: 679/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.47% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h36m Epoch: 680/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.79% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h36m Epoch: 681/1000 | Time: 0m09s (Train 0m09s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.32% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h36m Epoch: 682/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.27% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h37m Epoch: 683/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h37m Epoch: 684/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.15% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h37m Epoch: 685/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.98% | Val: Loss nan  Acc(top1) 78.00% | HA 86.00@55\n",
      "1h37m Epoch: 686/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.30% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h37m Epoch: 687/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h37m Epoch: 688/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.54% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h37m Epoch: 689/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h38m Epoch: 690/1000 | Time: 0m09s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 81.54% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h38m Epoch: 691/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.64% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h38m Epoch: 692/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.25% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h38m Epoch: 693/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.27% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h38m Epoch: 694/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.37% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h38m Epoch: 695/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h38m Epoch: 696/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.47% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h39m Epoch: 697/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.49% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h39m Epoch: 698/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 82.23% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h39m Epoch: 699/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 83.01% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h39m Epoch: 700/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 81.25% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h39m Epoch: 701/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h39m Epoch: 702/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.39  Acc 79.30% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h39m Epoch: 703/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.84% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h40m Epoch: 704/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 80.18% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h40m Epoch: 705/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.96% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "1h40m Epoch: 706/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.76% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h40m Epoch: 707/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.74% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h40m Epoch: 708/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 79.98% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h40m Epoch: 709/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.76% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h40m Epoch: 710/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h41m Epoch: 711/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 79.10% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h41m Epoch: 712/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 78.42% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h41m Epoch: 713/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.35% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h41m Epoch: 714/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.57% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h41m Epoch: 715/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h41m Epoch: 716/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.76% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h41m Epoch: 717/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.88% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h42m Epoch: 718/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.27% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h42m Epoch: 719/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 81.15% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h42m Epoch: 720/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.69% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h42m Epoch: 721/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.69% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h42m Epoch: 722/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 81.25% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h42m Epoch: 723/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.10% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h42m Epoch: 724/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.45% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h43m Epoch: 725/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h43m Epoch: 726/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.91% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h43m Epoch: 727/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.96% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h43m Epoch: 728/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.66% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h43m Epoch: 729/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.08% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h43m Epoch: 730/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.96% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h43m Epoch: 731/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h44m Epoch: 732/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.74% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h44m Epoch: 733/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.32% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h44m Epoch: 734/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 78.71% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h44m Epoch: 735/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.08% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h44m Epoch: 736/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.15% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h44m Epoch: 737/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.93% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h44m Epoch: 738/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 79.69% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h45m Epoch: 739/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.45% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h45m Epoch: 740/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.05% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h45m Epoch: 741/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h45m Epoch: 742/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.08% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h45m Epoch: 743/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.45% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h45m Epoch: 744/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.39  Acc 79.00% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h45m Epoch: 745/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.93% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h46m Epoch: 746/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 83.50% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h46m Epoch: 747/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.35% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h46m Epoch: 748/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 83.20% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h46m Epoch: 749/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h46m Epoch: 750/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.74% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h46m Epoch: 751/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 77.34% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h46m Epoch: 752/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h47m Epoch: 753/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.59% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h47m Epoch: 754/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.10% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h47m Epoch: 755/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 81.45% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h47m Epoch: 756/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.10% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h47m Epoch: 757/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h47m Epoch: 758/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.39  Acc 80.66% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h47m Epoch: 759/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 80.86% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h48m Epoch: 760/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h48m Epoch: 761/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.96% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h48m Epoch: 762/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h48m Epoch: 763/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.35% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h48m Epoch: 764/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.23% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h48m Epoch: 765/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.15% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h48m Epoch: 766/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 80.57% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h49m Epoch: 767/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.37% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h49m Epoch: 768/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.64% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h49m Epoch: 769/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.84% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h49m Epoch: 770/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 78.71% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h49m Epoch: 771/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.18% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h49m Epoch: 772/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.32% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h49m Epoch: 773/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.39  Acc 80.37% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h50m Epoch: 774/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h50m Epoch: 775/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.57% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h50m Epoch: 776/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 83.50% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h50m Epoch: 777/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h50m Epoch: 778/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.57% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h50m Epoch: 779/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.59% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h50m Epoch: 780/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.54% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h51m Epoch: 781/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "1h51m Epoch: 782/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.76% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h51m Epoch: 783/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h51m Epoch: 784/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 81.64% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h51m Epoch: 785/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 77.73% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h51m Epoch: 786/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.86% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h51m Epoch: 787/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 80.57% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h52m Epoch: 788/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.00% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "1h52m Epoch: 789/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.03% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h52m Epoch: 790/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 82.32% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h52m Epoch: 791/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 81.84% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h52m Epoch: 792/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.03% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h52m Epoch: 793/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.69% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h52m Epoch: 794/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.57% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h53m Epoch: 795/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.69% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h53m Epoch: 796/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.39% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h53m Epoch: 797/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.84% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h53m Epoch: 798/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.37% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h53m Epoch: 799/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h53m Epoch: 800/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h53m Epoch: 801/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.35% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h54m Epoch: 802/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 82.23% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h54m Epoch: 803/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 80.96% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h54m Epoch: 804/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.39% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h54m Epoch: 805/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.88% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h54m Epoch: 806/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.33  Acc 82.52% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h54m Epoch: 807/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.00% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h54m Epoch: 808/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.32  Acc 79.98% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h55m Epoch: 809/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.32% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h55m Epoch: 810/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.54% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "1h55m Epoch: 811/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 78.71% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h55m Epoch: 812/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h55m Epoch: 813/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.20% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h55m Epoch: 814/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.98% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h55m Epoch: 815/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 78.91% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h56m Epoch: 816/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.33  Acc 82.23% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h56m Epoch: 817/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.69% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "1h56m Epoch: 818/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 78.61% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h56m Epoch: 819/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.45% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h56m Epoch: 820/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.00% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "1h56m Epoch: 821/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.59% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "1h56m Epoch: 822/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h57m Epoch: 823/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.39% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "1h57m Epoch: 824/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.93% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h57m Epoch: 825/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 82.81% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h57m Epoch: 826/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 81.93% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h57m Epoch: 827/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.69% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h57m Epoch: 828/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 78.52% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "1h57m Epoch: 829/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 78.81% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h58m Epoch: 830/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.40  Acc 79.98% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h58m Epoch: 831/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.27% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h58m Epoch: 832/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "1h58m Epoch: 833/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.15% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h58m Epoch: 834/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 82.42% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h58m Epoch: 835/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 77.83% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h58m Epoch: 836/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.47% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h59m Epoch: 837/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.08% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h59m Epoch: 838/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 81.54% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h59m Epoch: 839/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h59m Epoch: 840/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.03% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "1h59m Epoch: 841/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.10% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h59m Epoch: 842/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 82.52% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "1h59m Epoch: 843/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 78.12% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h00m Epoch: 844/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 78.42% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h00m Epoch: 845/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.79% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h00m Epoch: 846/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h00m Epoch: 847/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.18% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h00m Epoch: 848/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.47% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h00m Epoch: 849/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h00m Epoch: 850/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.39  Acc 79.49% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h01m Epoch: 851/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.00% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h01m Epoch: 852/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.00% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h01m Epoch: 853/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 78.61% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h01m Epoch: 854/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 80.47% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h01m Epoch: 855/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.84% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "2h01m Epoch: 856/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.33  Acc 82.32% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h01m Epoch: 857/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 78.52% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h02m Epoch: 858/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 81.25% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h02m Epoch: 859/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.59% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h02m Epoch: 860/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.88% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "2h02m Epoch: 861/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.47% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h02m Epoch: 862/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.86% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h02m Epoch: 863/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.98% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h02m Epoch: 864/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.76% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h03m Epoch: 865/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 82.13% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h03m Epoch: 866/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 80.08% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "2h03m Epoch: 867/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 83.30% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h03m Epoch: 868/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 79.20% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h03m Epoch: 869/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.35% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h03m Epoch: 870/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 81.15% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h03m Epoch: 871/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 78.32% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h04m Epoch: 872/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 82.32% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h04m Epoch: 873/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.76% | Val: Loss nan  Acc(top1) 64.00% | HA 86.00@55\n",
      "2h04m Epoch: 874/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.40  Acc 79.30% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h04m Epoch: 875/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.45% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h04m Epoch: 876/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 78.71% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h04m Epoch: 877/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.54% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h04m Epoch: 878/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.10% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h04m Epoch: 879/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 78.03% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h05m Epoch: 880/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h05m Epoch: 881/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h05m Epoch: 882/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.57% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h05m Epoch: 883/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.52% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h05m Epoch: 884/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 80.96% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h05m Epoch: 885/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.79% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h05m Epoch: 886/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 81.05% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h06m Epoch: 887/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 81.45% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h06m Epoch: 888/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.47% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h06m Epoch: 889/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 79.79% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h06m Epoch: 890/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h06m Epoch: 891/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.38  Acc 78.61% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h06m Epoch: 892/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 81.35% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h06m Epoch: 893/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 80.18% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h07m Epoch: 894/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.34  Acc 82.91% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h07m Epoch: 895/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.37% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h07m Epoch: 896/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.35  Acc 82.32% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h07m Epoch: 897/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.57% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h07m Epoch: 898/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 80.66% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h07m Epoch: 899/1000 | Time: 0m09s (Train 0m09s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.37  Acc 81.35% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h07m Epoch: 900/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.0000000000000016e-05  Loss 0.36  Acc 80.18% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h08m Epoch: 901/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h08m Epoch: 902/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 80.47% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h08m Epoch: 903/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 82.32% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h08m Epoch: 904/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 78.32% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h08m Epoch: 905/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 79.98% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h08m Epoch: 906/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.35  Acc 80.57% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h08m Epoch: 907/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 80.47% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "2h09m Epoch: 908/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.39  Acc 80.66% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h09m Epoch: 909/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 81.05% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h09m Epoch: 910/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h09m Epoch: 911/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 80.57% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h09m Epoch: 912/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 82.42% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h09m Epoch: 913/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.84% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h09m Epoch: 914/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 82.81% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h10m Epoch: 915/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.35  Acc 82.23% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h10m Epoch: 916/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.35  Acc 80.47% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h10m Epoch: 917/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.34  Acc 80.86% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h10m Epoch: 918/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 80.57% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h10m Epoch: 919/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.35% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h10m Epoch: 920/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 80.18% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h10m Epoch: 921/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 80.57% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h11m Epoch: 922/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.33  Acc 82.52% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h11m Epoch: 923/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 77.54% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h11m Epoch: 924/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 78.42% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h11m Epoch: 925/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 79.10% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h11m Epoch: 926/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.39  Acc 81.05% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "2h11m Epoch: 927/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 79.49% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h11m Epoch: 928/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 78.91% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h12m Epoch: 929/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 82.81% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h12m Epoch: 930/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.35% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h12m Epoch: 931/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.35  Acc 81.93% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h12m Epoch: 932/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 82.23% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h12m Epoch: 933/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 79.49% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h12m Epoch: 934/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 79.00% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h12m Epoch: 935/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 79.79% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "2h13m Epoch: 936/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.74% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h13m Epoch: 937/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 80.66% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h13m Epoch: 938/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.35  Acc 82.62% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h13m Epoch: 939/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h13m Epoch: 940/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 79.20% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h13m Epoch: 941/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 81.35% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "2h13m Epoch: 942/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.39  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h14m Epoch: 943/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 82.81% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h14m Epoch: 944/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 81.05% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h14m Epoch: 945/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.45% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h14m Epoch: 946/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 79.88% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h14m Epoch: 947/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.34  Acc 80.76% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "2h14m Epoch: 948/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.34  Acc 82.52% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "2h14m Epoch: 949/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 80.08% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h15m Epoch: 950/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 80.57% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h15m Epoch: 951/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 80.96% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "2h15m Epoch: 952/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h15m Epoch: 953/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 80.27% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "save model to /home/sail/sound_project/sound_ai_v2/STEP/step_2/save_pt_model_s2/20240829_14/pruning_4C_time_2024082914_prunratio90.0/uec_4Cmodel_first_stage_prun_haacc_86.0_valacc72.0_tracc85.05859375_epoch_953_20240829171201.pt\n",
      "2h15m Epoch: 954/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.35  Acc 85.06% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h15m Epoch: 955/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 79.79% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h15m Epoch: 956/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 79.69% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h16m Epoch: 957/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h16m Epoch: 958/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.35  Acc 82.91% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h16m Epoch: 959/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 80.37% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h16m Epoch: 960/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 82.42% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h16m Epoch: 961/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h16m Epoch: 962/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 80.66% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h16m Epoch: 963/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.39  Acc 79.39% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h17m Epoch: 964/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.34  Acc 81.93% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h17m Epoch: 965/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 79.98% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h17m Epoch: 966/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.35  Acc 81.05% | Val: Loss nan  Acc(top1) 76.00% | HA 86.00@55\n",
      "2h17m Epoch: 967/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 79.59% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h17m Epoch: 968/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 80.08% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h17m Epoch: 969/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.39  Acc 78.71% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h17m Epoch: 970/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 78.12% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h18m Epoch: 971/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.54% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h18m Epoch: 972/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.39  Acc 77.73% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h18m Epoch: 973/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.15% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h18m Epoch: 974/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 79.79% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h18m Epoch: 975/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 78.52% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h18m Epoch: 976/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.45% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h18m Epoch: 977/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.34  Acc 79.69% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h19m Epoch: 978/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.34  Acc 81.05% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h19m Epoch: 979/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.34  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h19m Epoch: 980/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.34  Acc 81.15% | Val: Loss nan  Acc(top1) 66.00% | HA 86.00@55\n",
      "2h19m Epoch: 981/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 80.76% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h19m Epoch: 982/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 82.13% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h19m Epoch: 983/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.05% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h19m Epoch: 984/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 79.79% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h20m Epoch: 985/1000 | Time: 0m09s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 82.32% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h20m Epoch: 986/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.39  Acc 78.03% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h20m Epoch: 987/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.35  Acc 79.88% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h20m Epoch: 988/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 79.59% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h20m Epoch: 989/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 79.30% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h20m Epoch: 990/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 80.76% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h20m Epoch: 991/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.35  Acc 82.62% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h21m Epoch: 992/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 80.66% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h21m Epoch: 993/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.38  Acc 80.27% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h21m Epoch: 994/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.74% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h21m Epoch: 995/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.35  Acc 80.86% | Val: Loss nan  Acc(top1) 72.00% | HA 86.00@55\n",
      "2h21m Epoch: 996/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 78.52% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h21m Epoch: 997/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 78.32% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "2h21m Epoch: 998/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 81.25% | Val: Loss nan  Acc(top1) 74.00% | HA 86.00@55\n",
      "2h22m Epoch: 999/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.36  Acc 79.59% | Val: Loss nan  Acc(top1) 70.00% | HA 86.00@55\n",
      "2h22m Epoch: 1000/1000 | Time: 0m08s (Train 0m08s  Val 0m00s) | Train: LR 5.000000000000001e-06  Loss 0.37  Acc 79.10% | Val: Loss nan  Acc(top1) 68.00% | HA 86.00@55\n",
      "Execution finished in: 2h22m\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa199e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
